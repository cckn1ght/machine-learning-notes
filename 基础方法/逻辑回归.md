# 基础机器学习方法
@(MLNotes)

##Logistic Regression
分类问题是另一种常见的机器学习问题，跟回归问题不同，用类似于线性回归这样的预测连续数值的方式预测不同的类别会导致很奇怪的问题。因此，我们使用 Logistic Regression 方式来对问题进行分类。

首先我们先对 Logistic Regression 这个名字进行一下解释，中文直译就是逻辑回归。其实不仅中文直译很奇怪，这个词英文本身也很奇怪，作为一个分类算法，它的名字里却带有「回归」这个词。但是我觉得这个「回归」是有含义的，它表明了这个算法其实跟线性回归有一定的联系，我们先看多特征线性回归的假设函数（hypothesis function）：

$$h_\theta(x)=\theta^Tx$$

其中 $\theta$ 和 x 分别代表两个多维向量，数据的特征数加一就是维度数，$\theta$ 转置乘以 x 就是这两个向量的点积（dot product）。如果特征数是1，那这个函数就代表平面上的一条线，这就是线性回归的名称由来。并且这条线的取值范围是从负无穷到正无穷，除非它的斜率是0。但是现在我们想要分类，不想要预测连续的无止境的结果，分类需要的结果只是0或者1。

因此我们需要把上面的假设函数给“逻辑化”。就是说，用 Logistic 函数来处理上面的假设函数。我们先看 Logistic 函数的定义：

$$g(z)=\frac{1}{1+e^{-z}}$$

这个函数也叫做 Sigmoid 函数。我们看一眼这个函数长得样子就知道为什么要用到这个函数了。

![Logistic Function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/600px-Logistic-curve.svg.png)

它的取值区间是0到1，完美地符合了我们对于分类函数的需求。有同学要问了，这特么哪里完美了，这也是连续的一条线啊，只不过不是直线而是曲线，并没有告诉我们该如何做出判断。其实这一点可以通过一个简单的例子来解释，比如你真在买盒饭，面前摆了两种都是你特别喜欢的盒饭，但是身上只有买一份的钱。这个时候你的心里一定是摇摆的，就像上图中 x 轴在0坐标的时候一样，一会往右偏一点，一会往左偏一点。最终你会做出决定，即使还有对另一份盒饭的不舍，默默地在心里对自己说，”明天再来吃你“。这个时候你的心境就像是处在图中 x 轴零点零几的位置，你没有百分百地想要哪一份盒饭，但是你的选择只能是其中一个。因此，在利用 Logistic 函数进行分类的时候，只要函数值大于等于 0.5时，我们预测结果是1，否则，预测结果为0。

接下来我们把线性回归的假设函数当成一个变量并应用在 Sigmoid 函数上面：

$$g(h_\theta(x))=\frac{1}{1+e^{-\theta^Tx}}$$

这个函数的右边就是我们 Logistic Regress 的假设函数。这个函数的取值范围也是0到1。并且函数值在 $h_\theta(x)$（也就是 $\theta^Tx$ ）大于等于0时大于等于0.5，反之小于0.5。因此 $\theta^Tx=0$ 这个函数也称为**决策边界**。证明很简单，只需要把 $h_\theta(x)$ 想成 z，并代入上图中就可。
有了假设函数之后，我们需要定义代价函数，跟线性回归直觉式的代价函数不同，Logistic Regression 的代价函数比较复杂，通过统计学上的最大似然法可以得出，得到过程在此不详细解释，直接给出公式：

$$J(\theta)=\frac{1}{m}\sum^m_{i=1}Cost(h_\theta(x^{(i)}, y^{(i)}))$$
$$=-\frac{1}{m}[\sum^m_{i=1}y^{(i)}\log{h_\theta(x^{(i)})+(1-y^{(i)})\log{(1-h_\theta(x^{(i)})}}]$$

这个函数跟线性回归的代价函数一样，是个 convex 函数，convex 类型的函数值只存在一个局部最小值，即全局最小值。因此这个函数的最小值可以使用梯度递减函数来得到。关于 Logistic Regression 就讲到这里，梯度递减的方法另写一篇展开。

在此先列出它的梯度递减公式：

$$\theta_j := \theta_j - \alpha\sum^m_{i=1}{(h_\theta(x^{(i)})-y^{(i)}){x_j}^{(i)}}$$

我们可以发现它其实跟线性回归的过程一模一样，但是我们要注意这里的 $h_\theta(x)$ 已经不是线性函数了，指的是 $h_\theta(x)=\frac{1}{1+e^{-{\theta^Tx}}}$
